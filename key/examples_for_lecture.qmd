---
title: 'Datasets for examples and lab'
author: "Casey Oâ€™Hara"
date: "2024-02-15"
format: 
  html:
    embed-resources: true
    code-fold: true
    toc: true
execute:
  warning: false
  message: false
---

```{r setup}
library(tidyverse)
library(here)
library(tidymodels)
library(GGally)
library(jtools)
library(AICcmodavg)
```

# Overview

Using possum data, plastics in Lahore data, and penguins data, create figures for lecture.

# Sheep data

Allison's original example used a fictional dataset of male sheep success in finding a mate, as a function of their weight.  Let's recreate that.

```{r}
set.seed(12)
sheep_mate_df <- data.frame(success = sample(c(1, 0), 67, replace = TRUE)) %>%
  mutate(weight = rnorm(mean = 140, sd = 15, n = n()) + 30 * success,
         weight = round(weight, 1))

knitr::kable(head(sheep_mate_df))

ggplot(sheep_mate_df, aes(x = weight, y = success)) +
  geom_jitter(size = 2, alpha = .7, height = .02, width = 0) +
  theme_minimal() +
  labs(x = 'Sheep weight (lbs)',
       y = 'Mating success')
```

```{r sheep linear model}
lm1 <- lm(success ~ weight, data = sheep_mate_df)
ggplot(sheep_mate_df, aes(x = weight, y = success)) +
  geom_jitter(size = 2, alpha = .7, height = .02, width = 0) +
  geom_abline(intercept = lm1$coefficients[1],
              slope = lm1$coefficients[2], color = 'red') +
  theme_minimal() +
  scale_y_continuous(breaks = c(0, 1)) +
  labs(x = 'Sheep weight (lbs)',
       y = 'Mating success')
```

linear model not great for predicting categorical values.

## Binary logistic regression

Plotting as a straight line, the y axis is essentially $\ln\left(\frac{p_M}{1-p_M}\right)$
```{r}
sheep_blr1 <- glm(success ~ weight, data = sheep_mate_df, family = 'binomial')
summary(sheep_blr1)

ggplot(sheep_mate_df, aes(x = weight, y = success)) +
  geom_jitter(size = 2, alpha = .7, height = .02, width = 0) +
  geom_abline(intercept = sheep_blr1$coefficients[1],
              slope = sheep_blr1$coefficients[2], color = 'red') +
  theme_minimal() +
  scale_y_continuous(breaks = c(0, 1)) +
  labs(x = 'Sheep weight (lbs)',
       y = 'log odds: ln p/(1-p)')
```

Y axis labeled with log odds
```{r}
logodds_breaks <- log(c(.01, .1, 1, 10, 100))
logodds_labels <- c('log(1:100)', 'log(1:10)', 'log(1:1)', 'log(10:1)', 'log(100:1)')
ggplot(sheep_mate_df, aes(x = weight, y = success)) +
  geom_jitter(size = 2, alpha = .7, height = .02, width = 0) +
  geom_abline(intercept = sheep_blr1$coefficients[1],
              slope = sheep_blr1$coefficients[2], color = 'red') +
  theme_minimal() +
  scale_y_continuous(breaks = logodds_breaks, 
                     labels = logodds_labels, 
                     limits = range(logodds_breaks)) +
  labs(x = 'Sheep weight (lbs)',
       y = 'log odds: ln p/(1-p)')

```

```{r}
pred_blr1 <- data.frame(weight = 100:220) %>%
  mutate(prob_success = predict(sheep_blr1, ., type = 'response'))

ggplot(sheep_mate_df, aes(x = weight, y = success)) +
  geom_jitter(size = 2, alpha = .7, height = .02, width = 0) +
  geom_line(data = pred_blr1, aes(y = prob_success), color = 'red') +
  theme_minimal() +
  scale_y_continuous(breaks = c(0, 1), 
                     labels = c(0, 1)) +
  labs(x = 'Sheep weight (lbs)',
       y = 'Probability of success')
```

# Lahore plastics data

![](`r here('data/lahore_plastics/Badshahi_Mosque.jpg')`)

## Metadata:

This dataset, "Plastic Perception in Lahore," serves as a comprehensive snapshot of the city's attitude towards plastic pollution. Collected by our dedicated team of university fellows during January 2023, this raw dataset offers unfiltered insights into the perspectives and actions of Lahoris from different zones.

* Anoosha Tanseer (Owner)
* Qandeel Fatima (Editor)
* Tooba Noor (Editor)

Downloaded 2024-02-15 from:
https://www.kaggle.com/datasets/anooshatanseer/plastic-pollution-in-lahore

**Columns:**

Meta Data
`Gender`: Gender of the respondent (e.g., Male, Female, Other).
`Age`: Age of the respondent.
`Residence`: Specific zone or area within Lahore where the respondent resides. The values in this column represent the different zones from which the data was collected.
`Education`: Educational background of the respondent.
`Occupation`: Current occupation of the respondent.
`Environmental_Affect`: It contains Likert scale values about whether respondent agree or not that plastic is affectig our environment.
`Everyday_Affect`: How plastic pollution affects the respondent in daily life.
`Time_to_Degrade`: Perception of the time it takes for plastic to degrade.
`Recycling_Symbol`: Awareness and understanding of the recycling symbol.
`Microplastic_in_Food`: Awareness of microplastics in food.
`Chemical_Absorption`: Awareness of chemical absorption from plastics.
`Recycle_Plastic`: Participation in recycling plastic.
`Product_Use`: Usage of products made from plastic.
`Usage_Reason`: Reasons for using plastic products.
`Plastic_over_Other`: Preference for plastic over other materials.
`Purchase_Behaviour`: Behavior related to purchasing plastic products.
`Ensure_Biodegradable`: Willingness to ensure products are biodegradable.
`MicrowaveSafe`: Consideration of microwave safety in plastic usage.
`TakeYourBag`: Carrying reusable bags in advance instead of plastic.
`Location_of_Usage`: Places where plastic is used.
`Food_in_Bag`: Storing food in plastic bags.
`Bag_Reuse`: Reusing plastic bags.
`Bottle_Reuse`: Reusing plastic bottles.
`Cleaning_Responsibility`: Responsibility for cleaning plastic waste.
`Dispose_Outdoor`: Disposal behavior for plastic waste outdoors.
`Waste_Around_You`: Awareness of waste in the surrounding environment.
`Disposal_Behaviour`: Overall behavior towards the disposal of plastic.
`Special_Bins`: Utilization of special bins for plastic disposal.
`Negative_Impact`: Perception of negative impacts associated with plastic.
`Teaching_Kids`: Initiatives taken to educate children about plastic.
`Awareness_Source`: Sources of awareness regarding plastic pollution.
`Way_to_Reduce`: Strategies perceived to reduce plastic usage.
`Willing_to_Adopt`: Willingness to adopt alternatives to plastic.
`Alternatives`: Awareness and consideration of plastic alternatives.
`Survival_without_Plastic`: Perception of survival without using plastic.


## Analysis

Let's create a model to predict willingness to carry reusable bags based on a few predictors.  Let's look at `Waste_Around_You`, `Environmental_Affect`, `Gender`, `Age`, `Education`, `Negative_Impact`, and `Survival_.  Let's clean up a few of those: let's turn Likert into numbers centered on zero, and others into binary.

```{r load data}
likert <- c('strongly disagree', 'disagree', 'neutral', 'agree', 'strongly agree')

lahore_df_raw <- read_delim(here('data/lahore_plastics/survey_data.csv'), 
                        delim = ';',
                        show_col_types = FALSE) %>%
  janitor::clean_names() 

lahore_df <- lahore_df_raw %>%
  select(takebag = takeyourbag, adopt = willing_to_adopt, pick_it_up = waste_around_you, 
         symbol = recycling_symbol,
         environmental_affect, gender, age, education, survival = survival_without_plastic) %>%
  mutate(takebag = takebag %in% c('Always', 'Usually', 'Often'),
         education = ifelse(education == 'University', 'university', 'other'),
         age = factor(tolower(age), levels = c('young', 'middle aged', 'old')),
         gender = tolower(gender),
         symbol = symbol == 'Yes',
         environmental_affect = factor(tolower(environmental_affect),
                                       levels = likert) %>% as.integer(),
         survival = factor(tolower(survival),
                           levels = likert) %>% as.integer(),
         pick_it_up = str_detect(pick_it_up, 'pick it up'),
         adopt = str_detect(adopt, 'Agree'))

write_delim(lahore_df, delim = ';', here('data/lahore_plastics/survey_data_clean.csv'))
```

### check models

This model includes easily observable variables as potential predictors.

```{r}
f1 <- adopt ~ gender  + takebag + environmental_affect + education + age + survival

lahore_blr1 <- glm(formula = f1,
                   data = lahore_df,
                   family = "binomial")

# possum_blr1
 
summary(lahore_blr1)
 
# Get a tidy version w/ broom:
blr1_tidy <- broom::tidy(lahore_blr1)
```

But log odds are challenging to interpret. Let's find actual *probabilities* associated with a person being willing to adopt other practices

Adding `type.predict = "response"` here converts the log odds (link), the default reported, to the probability of being willing to adopt for each observation.

```{r}
blr1_fitted <- lahore_blr1 %>%
  broom::augment(type.predict = "response")
```

Look at the outcome data frame.

```{r}
blr1_predict <- lahore_df %>%
  mutate(adopt_prob = predict(lahore_blr1, data = ., type = 'response')) %>%
  mutate(adopt_pred = adopt_prob > 0.50) %>%
  group_by(adopt, adopt_pred) %>%
  summarize(n = n())

knitr::kable(blr1_predict)
```


## Visualization of p(Chinstrap) by variable

The `jtools::effect_plot()` function provides some quick model plotting. Note: for more customized visualization of model predictions, you may want to create a new "test" data frame of theoretical values, then use the `predict()` function to append predicted probabilities before plotting in `ggplot()`.

```{r}
# For flipper length:
jtools::effect_plot(ad_chin_blr1,
        	pred = flipper_length_mm,
        	interval = TRUE,
        	y.label = "Probability of 'Chinstrap'")
 
# For body mass:
effect_plot(ad_chin_blr1,
        	pred = body_mass_g,
        	interval = TRUE,
          	y.label = "Probability of 'Chinstrap'")
```

## Predictions for new values with `predict()`

What is the probability that a female penguin weight 3410 g with a flipper length of 192 mm will be Chinstrap?

```{r}
ex_1 <- predict(ad_chin_blr1,
                data.frame(sex = "female",
                  body_mass_g = 3410,
                  flipper_length_mm = 192),
                # tell it type = 'response' to get prob, not log odds
                type = "response")
 
# Based on the model, the probability that this penguin is a Chinstrap is 0.4.
```

You can also feed in a new data frame, with multiple penguin observations, to get model probability estimates for more than one penguin:

```{r}
new_df <- data.frame(
  sex = c("male", "male", "female"),
  body_mass_g = c(3298, 4100, 3600),
  flipper_length_mm = c(212, 175, 180)
)
 
ex_2 <- predict(ad_chin_blr1,
            	    new_df,
            	    type = "response")
```


## e. Binary logistic regression - new model

From the ggpairs plot, we saw that bill length might be a good predictor.  Let's now try to predict penguin species as a function of just bill length...

```{r}
f2 <- species ~ bill_length_mm + body_mass_g


ad_chin_blr2 <- glm(formula = f2,
                    data = adelie_chinstrap,
                    family = "binomial")
```

Look at the model:
```{r}
ad_chin_blr2
 
summary(ad_chin_blr2)
 
# Get a tidy version w/ broom:
blr2_tidy <- broom::tidy(ad_chin_blr2)
```


Let's see if this makes sense based on a visual comparison:
```{r}
ggplot(adelie_chinstrap, aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(aes(color = species))
```

Let's visualize the results for this model like we did before:
``` {r}
effect_plot(ad_chin_blr2,
        	pred = bill_length_mm,
        	interval = TRUE,
        	y.label = "Probability of 'Chinstrap'")


effect_plot(ad_chin_blr2,
        	pred = body_mass_g,
        	interval = TRUE,
        	y.label = "Probability of 'Chinstrap'")


```

## Model selection

Let's compare the models using AICc and BIC
```{r}
AICcmodavg::aictab(list(ad_chin_blr1, ad_chin_blr2))
AICcmodavg::bictab(list(ad_chin_blr1, ad_chin_blr2))
```

And let's compare with a 10-fold cross-validation, using prediction accuracy as our metric.

``` {r}
set.seed(123)


n_folds <- 10
fold_vec <- rep(1:n_folds, length.out = nrow(adelie_chinstrap))
ad_chin_kfold <- adelie_chinstrap %>%
  mutate(fold = sample(fold_vec, size = n(), replace = FALSE))

```

# for-loop version (SKIP FOR LAB - include as reference)

```{r}
results_df <- data.frame()
pred_acc <- function(x, y) {
  accurate <- ifelse(x == y, 1, 0)
  return(mean(accurate, na.rm = TRUE))
}

for(i in 1:n_folds) {
  kfold_test <- ad_chin_kfold %>%
    filter(fold == i)
  kfold_train <- ad_chin_kfold %>%
    filter(fold != i)
  
  kfold_blr1 <- glm(f1, data = kfold_train, family = 'binomial')
  kfold_blr2 <- glm(f2, data = kfold_train, family = 'binomial')
  kfold_pred <- kfold_test %>%
    mutate(blr1 = predict(kfold_blr1, kfold_test, type = 'response'),
           blr2 = predict(kfold_blr2, ., type = 'response')) %>%
    mutate(pred1 = ifelse(blr1 > 0.50, 'Chinstrap', 'Adelie'),
           pred2 = ifelse(blr2 > 0.50, 'Chinstrap', 'Adelie'))
  kfold_accuracy <- kfold_pred %>%
    summarize(blr1_acc = pred_acc(species, pred1),
              blr2_acc = pred_acc(species, pred2))
  
  results_df <- bind_rows(results_df, kfold_accuracy)
}


results_df %>%
  summarize(blr1_acc = mean(blr1_acc),
            blr2_acc = mean(blr2_acc))
```

# purrr::map version: returns a list

```{r}
x_vec <- 1:10

thing <- purrr::map(.x = x_vec, # a sequence (vector, list)
                    .f = sqrt)  # name of a function (without parens)

my_funct <- function(x, y, z) {
  return((x - y) ^ z)
}

thing2 <- purrr::map(.x = x_vec,      # a sequence (for first arg of function)
                     .f = my_funct,   # name of a function to apply
                     y = 2, z = 3)    # additional parameters (for other args)
```

``` {r}
# function to calculate accuracy, given a "truth" vector and "prediction" vector
pred_acc <- function(x, y) {
  accurate <- ifelse(x == y, 1, 0)
  
  return(mean(accurate, na.rm = TRUE))
}

# function to calculate accuracy of BLR of one fold (training and testing)
calc_fold <- function(i, fold_df, f) {
  kfold_test <- fold_df %>%
    filter(fold == i)
  kfold_train <- fold_df %>%
    filter(fold != i)
  
  kfold_blr <- glm(f, data = kfold_train, family = 'binomial')
  kfold_pred <- kfold_test %>%
    mutate(blr = predict(kfold_blr, kfold_test, type = 'response')) %>%
    mutate(pred = ifelse(blr > 0.50, 'Chinstrap', 'Adelie'))
  
  kfold_accuracy <- kfold_pred %>%
    summarize(blr_acc = pred_acc(species, pred)) # using my other function
  
  return(kfold_accuracy)
}

n_folds <- 10

results1_purrr_df <- purrr::map(.x = 1:n_folds, # sequence of fold numbers
                                .f = calc_fold, # function
                                fold_df = ad_chin_kfold, # additional argument to calc_fold()
                                f = f1) %>%              # additional argument to calc_fold()
  bind_rows() %>%
  mutate(mdl = 'f1')

results2_purrr_df <- purrr::map(.x = 1:n_folds, .f = calc_fold, 
                               fold_df = ad_chin_kfold,
                               f = f2) %>%
  bind_rows() %>%
  mutate(mdl = 'f2')

results_purrr_df <- bind_rows(results1_purrr_df, results2_purrr_df) %>%
  group_by(mdl) %>%
  summarize(mean_acc = mean(blr_acc))

results_purrr_df
```

Which model seems best?  Does this agree with AIC and BIC selection?


# Tidymodels flow

See https://www.tidymodels.org/ for tons of details and tutorials!  Tidymodels (and parsnip) packages clean up and standardize the output from hundreds of different modeling functions from dozens of different modeling packages.  For example, binomial logistic regression algorithms show up in quite a few different modeling packages, but the arguments and outputs differ from package to package - annoying!

Not going to get into: "recipes" for pre-processing, "workflows" 

## Tidymodels basic

```{r}
### Set the model type
?logistic_reg ### note glm is the default engine

blr_model <- logistic_reg() %>% ### also linear_reg, rand_forest, etc
  set_engine('glm')

### basic regression
blr_tidyfit_f1 <- blr_model %>%
  fit(f1, data = adelie_chinstrap)
blr_tidyfit_f2 <- blr_model %>%
  fit(f2, data = adelie_chinstrap)

### query the fitted models
blr_tidyfit_f1
blr_tidyfit_f2

### examine different outputs to see how well the models fit
blr_tidyfit_f1 %>%
  tidy()

blr_tidyfit_f1 %>%
  glance()

```

## Tidymodels crossfold validation

```{r}
### set seed for reproducibility! here to set the folds
set.seed(345)

tidy_folds <- vfold_cv(adelie_chinstrap, v = 10)
tidy_folds

### use a workflow that bundles the logistic model and a formula
# blr_model <- logistic_reg() %>%
#   set_engine('glm')

blr_tidy_wf1 <- workflow() %>%
  add_model(blr_model) %>%
  add_formula(f1)

blr_tidy_cv_f1 <- blr_tidy_wf1 %>%
  fit_resamples(tidy_folds)

### use functions from the tune package to extract metrics
collect_metrics(blr_tidy_cv_f1)
#   .metric  .estimator  mean     n std_err .config             
#   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               
# 1 accuracy binary     0.828    10 0.00739 Preprocessor1_Model1
# 2 roc_auc  binary     0.902    10 0.00808 Preprocessor1_Model1

### We'll talk about roc_auc next week!


### Repeat for model 2 - let students do this on their own!
blr_tidy_wf2 <- workflow() %>%
  add_model(blr_model) %>%
  add_formula(f2)

blr_tidy_cv_f2 <- blr_tidy_wf2 %>%
  fit_resamples(tidy_folds)

### use functions from the tune package to extract metrics
collect_metrics(blr_tidy_cv_f2)

```

## Area under the curve!

Receiver Operating Characteristic Curve (ROC Curve) compares the diagnostic ability of a binary classifier (like logistic regression) based on the discrimination threshold.  Up to now (and for homework) we've been using a 50% threshold by default.  The ROC can tell us tradeoffs between true positive rate and false positive rate as we change the threshold, and also can give a great indication of model quality.

It seems like model 2 is far better than model 1 in this instance.

```{r}
### This is copied from above, for reference
# blr_model <- logistic_reg() %>% ### also linear_reg, rand_forest, etc
#   set_engine('glm')
# 
# ### basic regression
# blr_tidyfit_f1 <- blr_model %>%
#   fit(f1, data = adelie_chinstrap)
# blr_tidyfit_f2 <- blr_model %>%
#   fit(f2, data = adelie_chinstrap)

blr_f1_pred <- adelie_chinstrap %>%
  mutate(predict(blr_tidyfit_f1, .),
         predict(blr_tidyfit_f1, ., type = 'prob'))

blr_f1_pred %>%
  roc_curve(truth = species, .pred_Adelie) %>%
  autoplot()

blr_f1_pred %>%
  roc_auc(truth = species, .pred_Adelie)

### Students repeat for blr_tidyfit_f2 and compare!
blr_f2_pred <- adelie_chinstrap %>%
  mutate(predict(blr_tidyfit_f2, .),
         predict(blr_tidyfit_f2, ., type = 'prob'))

blr_f2_pred %>%
  roc_curve(truth = species, .pred_Adelie) %>%
  autoplot()

blr_f2_pred %>%
  roc_auc(truth = species, .pred_Adelie)

```

# End Part 1

